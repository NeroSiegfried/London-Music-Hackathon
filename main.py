# -*- coding: utf-8 -*-
"""
Music Performance Feedback Generator using AI

This script serves as a proof-of-concept for the ABRSM challenge, demonstrating how
AI can be used to provide feedback on music performances.

It works in a four-step pipeline:
1.  GENERATE REFERENCE: A "perfect" version of a known piece ("Twinkle, Twinkle, Little Star")
    is created programmatically as both a MIDI file and a WAV audio file. This serves as our ground truth.
2.  ANALYZE PERFORMANCE: The user provides their own audio recording of the piece. The script
    analyzes this audio to extract pitch and rhythmic information (onsets).
3.  COMPARE & REPORT: The script compares the user's performance data against the reference
    data, calculating deviations in pitch and timing. It compiles these findings into a
    structured JSON report.
4.  GENERATE FEEDBACK: The JSON report is sent to the Google Gemini API with a carefully
    crafted prompt, asking it to act as a music examiner and provide encouraging,
    constructive feedback in natural language.

DESIGN CHOICES:
-   **Language & Libraries**: Python was chosen for its mature ecosystem of libraries for
    audio processing, scientific computing, and web requests.
    -   `librosa`: The industry standard for audio and music analysis in Python.
    -   `numpy`: The fundamental package for numerical computation, essential for handling audio data.
    -   `scipy`: Used here for writing the generated audio to a .wav file.
    -   `mido`: A lightweight and straightforward library for creating the reference MIDI file.
    -   `requests`: A simple and robust library for making the HTTP request to the Gemini API.
-   **Command-Line Interface (CLI)**: A CLI is ideal for a hackathon. It's fast to develop and
    clearly demonstrates the core functionality without the overhead of building a full GUI.
    `argparse` is used to make the CLI user-friendly.
-   **Self-Contained Reference**: The reference music is generated by the script itself. This is a
    key design choice to make the program self-sufficient and easy for judges to run without
    needing to download separate asset files.
-   **Heuristic Alignment**: Instead of a complex Dynamic Time Warping (DTW) algorithm, this script
    uses a simpler heuristic to align the performance with the reference. It matches each reference
    note to the closest performed note in time. This is more robust for a PoC and less prone to
    errors from background noise or flawed recordings.
-   **Structured Data (JSON)**: The analysis results are converted to a clean JSON object. This
    decouples the analysis from the feedback generation. It makes the data machine-readable and
    provides a clean, structured input for the LLM, leading to more reliable and consistent feedback.
-   **Prompt Engineering**: The core of the "AI" feedback is the system prompt sent to the Gemini
    API. Significant effort was put into crafting a persona (a friendly ABRSM examiner) and
    setting clear rules (be constructive, avoid jargon) to ensure the generated feedback is
    high-quality, appropriate, and genuinely useful.
-   **API Key Management**: The script prioritizes environment variables for API keys, which is a security
    best practice. It also allows passing the key as an argument for convenience during testing.

REQUIREMENTS:
pip install librosa numpy scipy mido requests

HOW TO RUN:
1.  Set your Google AI Studio API key as an environment variable:
    -   On macOS/Linux: export GOOGLE_API_KEY='your_api_key_here'
    -   On Windows: set GOOGLE_API_KEY='your_api_key_here'
2.  Run the script from your terminal, passing the path to your audio file:
    python music_feedback_analyzer.py /path/to/your_performance.wav
"""

import os
import json
import argparse
import numpy as np
import librosa
import mido
import requests
from scipy.io import wavfile

# --- Configuration ---
# Design Choice: Using a dictionary for the melody makes it readable and easy to modify.
# 'pitch' is the MIDI note number (60 = Middle C), 'duration' is in quarter notes.
TWINKLE_MELODY = [
    {'pitch': 60, 'duration': 0.25}, {'pitch': 60, 'duration': 0.25},  # Twinkle twinkle
    {'pitch': 67, 'duration': 0.25}, {'pitch': 67, 'duration': 0.25},  # little star
    {'pitch': 69, 'duration': 0.25}, {'pitch': 69, 'duration': 0.25},  # How I wonder
    {'pitch': 67, 'duration': 0.5},                                  # what you are
    {'pitch': 65, 'duration': 0.25}, {'pitch': 65, 'duration': 0.25},  # Up above the
    {'pitch': 64, 'duration': 0.25}, {'pitch': 64, 'duration': 0.25},  # world so high
    {'pitch': 62, 'duration': 0.25}, {'pitch': 62, 'duration': 0.25},  # Like a diamond
    {'pitch': 60, 'duration': 0.5},                                  # in the sky
]
TEMPO_BPM = 100
SAMPLE_RATE = 22050
REFERENCE_PREFIX = "twinkle_reference"

# --- 1. GENERATE REFERENCE DATA ---

def create_reference_data(melody, tempo, sr, prefix):
    """
    Generates and saves a reference MIDI file and a WAV audio file from a melody definition.

    Design Choice: This function makes the script self-contained. By generating the
    "perfect" performance, we don't need to bundle media files. This is great for
    portability and demonstration.
    """
    midi_path = f"{prefix}.mid"
    wav_path = f"{prefix}.wav"

    if os.path.exists(midi_path) and os.path.exists(wav_path):
        # print("Reference files already exist.")
        return

    print("Generating reference MIDI and WAV files...")
    # --- MIDI Generation ---
    # Design Choice: mido is a lightweight library perfect for simple MIDI creation.
    mid = mido.MidiFile()
    track = mido.MidiTrack()
    mid.tracks.append(track)

    ticks_per_beat = mid.ticks_per_beat
    seconds_per_beat = 60.0 / tempo

    for note in melody:
        duration_ticks = int(mido.second2tick(note['duration'] * 4 * seconds_per_beat, ticks_per_beat, tempo))
        track.append(mido.Message('note_on', note=note['pitch'], velocity=64, time=0))
        track.append(mido.Message('note_off', note=note['pitch'], velocity=64, time=duration_ticks))
    mid.save(midi_path)

    # --- WAV Generation ---
    # Design Choice: Synthesizing a simple sine wave is a quick and effective way
    # to create a clean audio reference for pitch analysis.
    total_duration_seconds = sum(n['duration'] for n in melody) * 4 * seconds_per_beat
    wav_data = np.zeros(int(total_duration_seconds * sr))
    current_time = 0.0

    for note in melody:
        frequency = librosa.midi_to_hz(note['pitch'])
        duration_samples = int(note['duration'] * 4 * seconds_per_beat * sr)
        
        t = np.linspace(0., duration_samples / sr, duration_samples, endpoint=False)
        note_wave = 0.5 * np.sin(2 * np.pi * frequency * t)

        start_sample = int(current_time * sr)
        end_sample = start_sample + len(note_wave)
        wav_data[start_sample:end_sample] += note_wave

        current_time += note['duration'] * 4 * seconds_per_beat

    # Normalize audio to prevent clipping
    wav_data = wav_data / np.max(np.abs(wav_data)) * 0.9
    wavfile.write(wav_path, sr, (wav_data * 32767).astype(np.int16))
    print(f"Reference files '{midi_path}' and '{wav_path}' created.")

# --- 2. ANALYZE PERFORMANCE AUDIO ---

def analyze_performance_audio(audio_path, sr):
    """
    Loads a user's audio performance and extracts pitch and onsets.

    Design Choice: `librosa.pyin` is chosen for pitch extraction as it's a robust
    algorithm that provides high-quality pitch tracking. `librosa.onset.onset_detect`
    is a standard tool for finding the start of musical notes (rhythm).
    """
    print(f"Analyzing performance file: {audio_path}...")
    try:
        y, _ = librosa.load(audio_path, sr=sr)
    except Exception as e:
        print(f"Error loading audio file: {e}")
        return None, None

    # Pitch extraction
    f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C6'))
    
    # Get timestamps for pitch values
    times = librosa.times_like(f0, sr=sr)

    # Onset detection (for rhythm)
    onset_frames = librosa.onset.onset_detect(y=y, sr=sr, units='frames')
    onset_times = librosa.frames_to_time(onset_frames, sr=sr)
    
    return f0, times, onset_times

# --- 3. COMPARE & GENERATE REPORT ---

def compare_and_generate_report(melody, tempo, performance_f0, performance_times, performance_onsets):
    """
    Compares the performance against the reference and creates a structured JSON report.

    Design Choice: A heuristic alignment approach is used here for simplicity and robustness.
    Instead of complex DTW, we find the closest performed note for each expected reference note.
    This is sufficient for a PoC and less prone to errors with noisy input. The output is a
    JSON object, which is a perfect machine-readable format for an LLM.
    """
    print("Comparing performance to reference and generating report...")
    report = {
        "piece_title": "Twinkle, Twinkle, Little Star",
        "overall_assessment": {},
        "note_details": []
    }

    seconds_per_beat = 60.0 / tempo
    reference_onsets = []
    current_time = 0.0
    for note in melody:
        reference_onsets.append(current_time)
        current_time += note['duration'] * 4 * seconds_per_beat

    pitch_deviations = []
    timing_deviations = []

    # Find the best matching performance onset for each reference note
    for i, ref_note in enumerate(melody):
        ref_onset = reference_onsets[i]
        ref_pitch_midi = ref_note['pitch']
        ref_duration = ref_note['duration'] * 4 * seconds_per_beat

        # Find the closest performance onset
        if len(performance_onsets) == 0: continue
        closest_onset_idx = np.argmin(np.abs(performance_onsets - ref_onset))
        perf_onset = performance_onsets[closest_onset_idx]

        # Calculate timing deviation
        timing_dev = (perf_onset - ref_onset) * 1000 # in ms
        timing_deviations.append(abs(timing_dev))

        # Find the average pitch during this note's duration
        start_idx = np.argmin(np.abs(performance_times - perf_onset))
        end_idx = np.argmin(np.abs(performance_times - (perf_onset + ref_duration * 0.9))) # Look at 90% of the note
        
        note_pitches_f0 = performance_f0[start_idx:end_idx]
        note_pitches_f0 = note_pitches_f0[~np.isnan(note_pitches_f0)] # remove silent frames

        if len(note_pitches_f0) > 0:
            avg_f0 = np.mean(note_pitches_f0)
            perf_pitch_midi = librosa.hz_to_midi(avg_f0)
            pitch_dev = (perf_pitch_midi - ref_pitch_midi) * 100 # in cents
            pitch_deviations.append(abs(pitch_dev))
        else:
            perf_pitch_midi = None
            pitch_dev = None
            
        report["note_details"].append({
            "note_index": i,
            "expected_pitch": librosa.midi_to_note(ref_pitch_midi),
            "timing_deviation_ms": round(timing_dev),
            "pitch_deviation_cents": round(pitch_dev) if pitch_dev is not None else "N/A (missed note)"
        })

    # Calculate overall stats
    if pitch_deviations:
        report["overall_assessment"]["avg_pitch_error_cents"] = round(np.mean(pitch_deviations))
    if timing_deviations:
        report["overall_assessment"]["avg_timing_error_ms"] = round(np.mean(timing_deviations))
    
    return json.dumps(report, indent=2)


# --- 4. GET FEEDBACK FROM LLM ---

def get_feedback_from_llm(report_json, api_key):
    """
    Sends the analysis report to the Gemini API and returns the generated feedback.

    Design Choice: Prompt engineering is the most critical part of this function.
    The system prompt gives the LLM a clear persona (ABRSM examiner), a tone (encouraging),
    and specific instructions (start with positives, explain simply). This ensures the
    output is high-quality and meets the challenge's requirements.
    """
    print("Sending report to Gemini API for feedback...")

    # Design Choice: The model 'gemini-2.5-flash-preview-05-20' is specified as it is a fast and capable model suitable for this task.
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}"

    # Design Choice: This system prompt is the "brain" of the feedback generator. It constrains the LLM
    # to a specific, helpful persona.
    system_prompt = (
        "You are a kind, encouraging, and professional music examiner for ABRSM. "
        "Your task is to provide constructive feedback to a music student based on a technical "
        "analysis of their performance of 'Twinkle, Twinkle, Little Star'. "
        "The user will provide a JSON report with technical data. Your feedback must follow these rules:\n"
        "1.  Your tone must be positive and encouraging.\n"
        "2.  Always start by mentioning a strength or something the student did well.\n"
        "3.  Translate the technical data into simple, easy-to-understand advice. "
        "DO NOT mention 'cents', 'milliseconds', 'ms', or 'Hz'. Instead, use terms like "
        "'slightly sharp', 'a little flat', 'rushed', 'a little late', or 'held the note nicely'.\n"
        "4.  Focus on the one or two most significant areas for improvement. Don't overwhelm the student.\n"
        "5.  Keep the feedback concise, around 3-4 paragraphs.\n"
        "6.  End with an encouraging closing statement."
    )
    
    user_query = (
        "Please generate feedback for the student based on the following performance report:\n\n"
        f"{report_json}"
    )

    payload = {
        "contents": [{"parts": [{"text": user_query}]}],
        "systemInstruction": {
            "parts": [{"text": system_prompt}]
        },
    }

    headers = {'Content-Type': 'application/json'}

    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=60)
        response.raise_for_status()  # Raise an exception for bad status codes
        result = response.json()
        
        # Safely extract the text
        candidate = result.get("candidates", [{}])[0]
        content = candidate.get("content", {}).get("parts", [{}])[0]
        feedback = content.get("text", "Error: Could not extract feedback from the API response.")
        
        return feedback
    except requests.exceptions.RequestException as e:
        return f"Error connecting to the API: {e}"
    except (KeyError, IndexError) as e:
        return f"Error parsing API response: {e}\nResponse received:\n{result}"


# --- MAIN EXECUTION ---

def main():
    """
    Main function to orchestrate the feedback generation process.
    """
    # Design Choice: argparse makes the script a proper command-line tool. It's user-friendly
    # and provides help messages automatically.
    parser = argparse.ArgumentParser(description="AI Music Performance Feedback Generator (ABRSM PoC)")
    parser.add_argument("audio_file", help="Path to the user's performance audio file (e.g., .wav, .mp3)")
    parser.add_argument("--api-key", help="Your Google AI Studio API key. (Alternatively, set GOOGLE_API_KEY environment variable)")
    args = parser.parse_args()

    api_key = args.api_key or os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        print("ERROR: Google API Key not found. Please provide it via the --api-key argument or set the GOOGLE_API_KEY environment variable.")
        return

    # 1. Generate Reference Data (if needed)
    create_reference_data(TWINKLE_MELODY, TEMPO_BPM, SAMPLE_RATE, REFERENCE_PREFIX)

    # 2. Analyze Performance
    f0, times, onsets = analyze_performance_audio(args.audio_file, SAMPLE_RATE)
    if f0 is None:
        return

    # 3. Compare and Report
    report = compare_and_generate_report(TWINKLE_MELODY, TEMPO_BPM, f0, times, onsets)
    print("\n--- Analysis Report (JSON sent to LLM) ---")
    print(report)
    print("-------------------------------------------\n")

    # 4. Generate Feedback
    feedback = get_feedback_from_llm(report, api_key)
    print("--- Your Personalised ABRSM Feedback ---")
    print(feedback)
    print("----------------------------------------")


if __name__ == "__main__":
    main()
